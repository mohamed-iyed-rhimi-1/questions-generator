# Database Configuration
# PostgreSQL connection string for the application
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/youtube_qa_db

# PostgreSQL credentials (used by Docker and migrations)
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=youtube_qa_db

# Application Ports
# Port for the FastAPI backend server
BACKEND_PORT=8000
# Port for the Vite development server
FRONTEND_PORT=5173

# ============================================================================
# AI Provider Configuration
# ============================================================================
# Choose between local AI models (privacy-focused, no API costs, requires GPU)
# or cloud-based APIs (faster, no local resources required, pay-per-use)

# Transcription Provider Configuration
# ----------------------------------------------------------------------------
# Provider for audio transcription: 'whisper' (local) or 'groq' (API)
# - whisper: Runs OpenAI Whisper locally (requires GPU for best performance)
# - groq: Uses Groq API for fast cloud-based transcription
TRANSCRIPTION_PROVIDER=groq

# Groq API Configuration (only required if TRANSCRIPTION_PROVIDER=groq)
# Get your API key from: https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here

# Groq model for transcription
# Options: whisper-large-v3 (recommended), whisper-large-v3-turbo
# See available models at: https://console.groq.com/docs/models
GROQ_MODEL=whisper-large-v3

# Local Whisper Configuration (only required if TRANSCRIPTION_PROVIDER=whisper)
# Whisper model to use for local transcription
# Options: turbo (recommended), tiny, base, small, medium, large, large-v3
WHISPER_MODEL=turbo

# Question Generation Provider Configuration
# ----------------------------------------------------------------------------
# Provider for question generation: 'ollama' (local) or 'openrouter' (API)
# - ollama: Uses local Ollama server for LLM inference (requires Ollama installed)
# - openrouter: Uses OpenRouter API for cloud-based LLM access
QUESTION_GENERATION_PROVIDER=openrouter

# OpenRouter API Configuration (only required if QUESTION_GENERATION_PROVIDER=openrouter)
# Get your API key from: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# OpenRouter model for question generation
# Options: openai/gpt-4o-mini (recommended, fast and affordable),
#          openai/gpt-4o (more capable but expensive),
#          anthropic/claude-3.5-sonnet (high quality),
#          meta-llama/llama-3.1-70b-instruct (open source)
# See all models at: https://openrouter.ai/models
OPENROUTER_MODEL=openai/gpt-4o-mini

# Optional: Your site URL for OpenRouter rankings and analytics
OPENROUTER_SITE_URL=

# Optional: Your site name for OpenRouter rankings
OPENROUTER_SITE_NAME=

# Local Ollama Configuration (only required if QUESTION_GENERATION_PROVIDER=ollama)
# Base URL for the Ollama API (local or remote)
OLLAMA_BASE_URL=http://localhost:11434

# Model to use for local question generation
# Options: llama3, mistral, codellama, iKhalid/ALLaM:7b (Arabic-focused)
OLLAMA_MODEL=iKhalid/ALLaM:7b

# Storage Configuration
# Path where downloaded videos and thumbnails will be stored
STORAGE_PATH=./storage

# Chunk Configuration
# ----------------------------------------------------------------------------
# Configuration for splitting large audio files into smaller chunks
# This enables processing of videos that exceed transcription provider limits

# Maximum chunk size in megabytes (default: 25.0)
# Audio files larger than this will be automatically split into chunks
# Set to match your transcription provider's file size limit (e.g., Groq has 25MB limit)
MAX_CHUNK_SIZE_MB=25.0

# Silence detection threshold in decibels (default: -35)
# Audio below this threshold is considered silence
# Lower values (e.g., -40) detect more silence, higher values (e.g., -30) are more strict
SILENCE_THRESHOLD_DB=-35

# Minimum silence duration in seconds (default: 0.3)
# Only silence periods longer than this will be used as split points
# Helps avoid splitting on brief pauses
MIN_SILENCE_DURATION=0.3

# Enable automatic chunking for large files (default: true)
# Set to 'false' to disable automatic chunking and process files as complete units
AUTO_CHUNK_ENABLED=true

# Delete original audio file after successful chunking (default: false)
# Set to 'true' to save disk space by removing the original file after chunks are created
# WARNING: Original file cannot be recovered if chunks are deleted
DELETE_ORIGINAL_AFTER_CHUNKING=false

# CORS Configuration
# Comma-separated list of allowed origins for CORS
# In production, replace with your actual frontend domain
# Include http://localhost for production nginx serving on port 80
CORS_ORIGINS=http://localhost:5173,http://localhost:3000,http://localhost

# Logging Configuration
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Use DEBUG for development, INFO for production
LOG_LEVEL=INFO

# Log format: json (structured, for production) or text (human-readable, for development)
# Use 'json' for production (parseable by log aggregators), 'text' for development
LOG_FORMAT=json

# Enable file-based log rotation (set to 'true' in production)
ENABLE_LOG_ROTATION=false

# Path where log files will be written (only used if rotation is enabled)
LOG_FILE_PATH=./logs/app.log
